{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset\n",
      "Dataset processed\n",
      "Loading embeddings\n",
      "Embeddings loaded\n",
      "Validation size:40429 and training size:363861\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a94784aa55d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Split to dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mX_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0mX_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpad_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a94784aa55d0>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mpad_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Split to dicts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/preprocessing/sequence.pyc\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an index"
     ]
    }
   ],
   "source": [
    "\"Input training file\"\n",
    "training_file_dir = \"train.csv\"\n",
    "df_train = pd.read_csv(training_file_dir)\n",
    "#All initial values, running this cell will reset all base variables\n",
    "word_to_int = {\"<UNK>\":0}\n",
    "int_to_word = {0:\"<UNK>\"}\n",
    "X_training  = df_train.as_matrix()\n",
    "\n",
    "vectors_dir = \"/home/anna/embeddings/gensim_format_glove.6B.300d.txt\"\n",
    "from gensim.models import KeyedVectors \n",
    "#Hyperparameters and initial settings\n",
    "embedding_dims = 300\n",
    "hidden_dims = 64\n",
    "gradient_clipping_norm = 1.25\n",
    "n_epoch = 25\n",
    "n_batch = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "from time import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Merge, Embedding, Input\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM, Bidirectional\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tokenize_question(q, lower= False):\n",
    "    '''Either use NLTK for this or use: https://github.com/myleott/ark-twokenize-py/blob/master/twokenize.py '''\n",
    "    try:\n",
    "        tokens = word_tokenize(q)\n",
    "    except UnicodeDecodeError:\n",
    "        tokens = word_tokenize(q.decode('utf-8'))\n",
    "    except:\n",
    "        return [\"<UNK>\"]\n",
    "    word_tokens = [word for word in tokens if word.isalpha()]  #only include words; not sure if best option\n",
    "    word_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "    if(lower):\n",
    "        word_tokens = map(lambda x: x.lower(), word_tokens) #converting all to lower case\n",
    "    return word_tokens\n",
    "\n",
    "seq_length_list = []\n",
    "\n",
    "def get_word_to_int_sequence(tokens):\n",
    "    '''Returns sequence and updates vocab'''\n",
    "    '''Does increasing number of functions impact performance?'''\n",
    "    seq = []\n",
    "    #global max_seq_length\n",
    "    for token in tokens:\n",
    "        if(token not in word_to_int):\n",
    "            word_to_int[token] = len(word_to_int)\n",
    "            int_to_word[len(word_to_int)] = token\n",
    "            seq.append(word_to_int[token])\n",
    "        else:\n",
    "            seq.append(word_to_int[token])\n",
    "    #if(len(seq)>max_seq_length):\n",
    "     #   max_seq_length = len(seq)\n",
    "    seq_length_list.append(len(seq))\n",
    "    return seq\n",
    "\n",
    "\n",
    "\n",
    "def process_dataset(dataset, dodeepcopy = True):\n",
    "    '''Input is a numpy array of questions\n",
    "    Output is an array of sequences according to word_to_int dict'''\n",
    "    if(not dodeepcopy):\n",
    "        dataset_mod = dataset\n",
    "    else:\n",
    "        dataset_mod = deepcopy(dataset)\n",
    "    \n",
    "    for i, row in enumerate(dataset_mod):\n",
    "        #print(i) #for debugging\n",
    "        q1,q2 = row[3],row[4]  #these correspond to the question\n",
    "        q1_tokens, q2_tokens = tokenize_question(q1),tokenize_question(q2)\n",
    "        q1_seq, q2_seq = get_word_to_int_sequence(q1_tokens), get_word_to_int_sequence(q2_tokens)\n",
    "        row[3],row[4] = q1_seq, q2_seq\n",
    "        \n",
    "    \n",
    "    return dataset_mod\n",
    "\n",
    "print(\"Processing dataset\")\n",
    "X_training = process_dataset(X_training, dodeepcopy= False)\n",
    "print(\"Dataset processed\")\n",
    "\n",
    "max_seq_length = int(round(np.mean(seq_length_list) + 2*np.std(seq_length_list)))\n",
    "\n",
    "\n",
    "'''Load embeddings'''\n",
    "print(\"Loading embeddings\")\n",
    "word_vectors = KeyedVectors.load_word2vec_format(vectors_dir, binary=False)\n",
    "word_embeddings = np.random.randn(len(word_to_int), embedding_dims)\n",
    "word_embeddings[0] = 0\n",
    "for word, i in word_to_int.items():\n",
    "    if(word in word_vectors.vocab):\n",
    "        word_embeddings[i] = word_vectors.word_vec(word)\n",
    "print(\"Embeddings loaded\")\n",
    "        \n",
    "validation_size = int(round((0.1)*X_training.shape[0]))\n",
    "training_size = X_training.shape[0] - validation_size\n",
    "print(\"Validation size:{} and training size:{}\".format(validation_size, training_size))\n",
    "\n",
    "X_qs = np.array(map(lambda x:[x[3],x[4]], X_training))\n",
    "Y = np.array(map(lambda x: [x[-1]], X_training))\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_qs, Y, test_size = validation_size)\n",
    "\n",
    "pad_seq = lambda x: sequence.pad_sequences(x,maxlen=max_seq_length)\n",
    "\n",
    "# Split to dicts\n",
    "X_training = {'left': pad_seq(X_train[:,0]), 'right': pad_seq(X_train[:,1])}\n",
    "X_validation = {'left': pad_seq(X_val[:,0]), 'right': pad_seq(X_val[:,1])}\n",
    "\n",
    "\n",
    "'''Final model'''\n",
    "\n",
    "def l1_dist_exp(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True)) #returns negative l1 norm exponent across batches\n",
    "\n",
    "q1_placeholder = Input(shape=(max_seq_length,), dtype =\"int32\")\n",
    "q2_placeholder = Input(shape=(max_seq_length,), dtype =\"int32\")\n",
    "embed_layer = Embedding(len(word_embeddings), embedding_dims, weights = [word_embeddings], input_length = max_seq_length, trainable = False)\n",
    "#Experiment with trainable but large corpus should not affect\n",
    "\n",
    "embedded_q1, embedded_q2 = embed_layer(q1_placeholder), embed_layer(q2_placeholder)\n",
    "\n",
    "siamese_LSTM = LSTM(hidden_dims)\n",
    "q1_enc, q2_enc = siamese_LSTM(embedded_q1), siamese_LSTM(embedded_q2)\n",
    "\n",
    "l1_dist = Merge(mode = lambda x: l1_dist_exp(x[0],x[1]), output_shape = lambda x: (x[0][0],1))([q1_enc, q2_enc])\n",
    "\n",
    "final_model = Model(inputs = [q1_placeholder, q2_placeholder], outputs = [l1_dist]) \n",
    "\n",
    "optimizer = Adadelta(clipnorm = gradient_clipping_norm)\n",
    "final_model.compile(loss = \"mean_squared_error\", optimizer = optimizer, metrics= ['accuracy'])\n",
    "\n",
    "t1 = time()\n",
    "trained_model = final_model.fit([X_training['left'], X_training['right']], Y_train, batch_size = n_batch, nb_epoch= n_epoch, validation_data = ([X_validation['left'], X_validation['right']], Y_val))\n",
    "print(\"Training finished.\\n {}epochs in {}\".format(n_epoch, time()-t1))\n",
    "\n",
    "'''Plot accuracy'''\n",
    "# Plot accuracy\n",
    "plt.plot(trained_model.history['acc'])\n",
    "plt.plot(trained_model.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(\"accyracy.png\")\n",
    "plt.close()\n",
    "# Plot loss\n",
    "\n",
    "plt.plot(trained_model.history['loss'])\n",
    "plt.plot(trained_model.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.savefig(\"loss.png\")\n",
    "\n",
    "\n",
    "'''Save model'''\n",
    "model_json = trained_model.to_json()\n",
    "with open(\"final_ma_lstm.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "trained_model.save_weights(\"final_ma_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mAVLinux\u001b[0m/     examples.desktop     lstm.py                  test_gpu.py\r\n",
      "dataset.txt  \u001b[01;34mFYP\u001b[0m/                 moremeaningful_name.txt  test.txt\r\n",
      "\u001b[01;34mDesktop\u001b[0m/     ipython_history.txt  \u001b[01;34mnltk_data\u001b[0m/               tflow.py\r\n",
      "\u001b[01;34mDocuments\u001b[0m/   \u001b[01;34mkeras\u001b[0m/               \u001b[01;34mPublic\u001b[0m/                  train.txt\r\n",
      "\u001b[01;34mDownloads\u001b[0m/   key.pgp              r8-test-all-terms.txt\r\n",
      "\u001b[01;34membeddings\u001b[0m/  \u001b[01;34mlawi\u001b[0m/                r8-train-all-terms.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
